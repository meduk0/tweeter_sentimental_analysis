FROM debian:bookworm-slim

WORKDIR /app

# Install Python and required tools
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    curl \
    procps \
    gcc \
    g++ \
    make \
    openjdk-17-jre-headless \
    && rm -rf /var/lib/apt/lists/*

# Verify Java installation
RUN java -version 2>&1

# Set Java environment variables - use the actual path from openjdk image
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

ENV PATH="${JAVA_HOME}/bin:${PATH}"

# Set JVM memory options for Spark - reduced for container environment
ENV _JAVA_OPTIONS="-Xmx1g -Xms256m -Duser.country=US -Duser.language=en -Djavax.security.auth.useSubjectCredsOnly=false"
ENV JAVA_TOOL_OPTIONS="-Xmx1g -Xms256m"

# Set Hadoop user (required for Spark)
ENV HADOOP_USER_NAME=root

# Copy requirements first for better caching
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir --break-system-packages -r requirements.txt

# Copy application files
COPY webapp/app.py .

# Copy Streamlit configuration
# The source folder is `.streamlit` inside the project `webapp` directory
COPY ./webapp/.streamlit/ ./.streamlit/

# Copy saved models and data (these should be generated by running the notebook first)
COPY ./models/ ./saved_models/

# Expose Streamlit port
EXPOSE 8501

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl --fail http://localhost:8501/_stcore/health || exit 1

# Set Streamlit configuration
ENV STREAMLIT_SERVER_PORT=8501
ENV STREAMLIT_SERVER_ADDRESS=0.0.0.0
ENV STREAMLIT_SERVER_HEADLESS=true
ENV STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
ENV STREAMLIT_LOGGER_LEVEL=error

# Disable file watcher to avoid inotify issues
ENV STREAMLIT_FILE_WATCHER_TYPE=none

# Run the application
CMD ["streamlit", "run", "app.py", "--logger.level=error"]
